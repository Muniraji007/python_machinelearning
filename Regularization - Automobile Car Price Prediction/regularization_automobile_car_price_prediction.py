# -*- coding: utf-8 -*-
"""Regularization - Automobile Car Price Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xttkO4au0y4CO_2W-81ZQqc3URPnFll2
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score

# Step 1: Load the dataset   and getting data in colab drive
df = pd.read_csv('/content/carsprediction.csv')

df.shape

# Step 2: Basic data cleaning
# Replace '?' with NaN for easier handling of missing values
# This is a common preprocessing step when datasets use '?' to represent missing data
# Using np.nan allows us to leverage pandas' built-in functions for handling missing values
df.replace('?', np.nan, inplace=True)
print(df.head())

# Convert columns with numeric data stored as strings to numeric type
numeric_cols = ['normalized-losses', 'width', 'height', 'engine-size', 'horsepower',
                'city-mpg', 'highway-mpg', 'price']

for col in numeric_cols:
    df[col] = pd.to_numeric(df[col])

# Drop rows where target 'price' is missing, as we cannot train without target
df.dropna(subset=['price'], inplace=True)



# prompt: For simplicity, fill missing numeric values with column mean


# Identify numeric columns that might still have missing values after the initial cleaning
numeric_cols_with_nan = df.select_dtypes(include=np.number).columns

# Fill missing values in numeric columns with the mean of the column
for col in numeric_cols_with_nan:
    if df[col].isnull().any():
        df[col].fillna(df[col].mean(), inplace=True)

print("\nDataFrame after filling missing numeric values with mean:")
print(df.head())

# Verify if there are still missing values
print("\nMissing values after filling:")
print(df.isnull().sum())

# Step 3: Separate features and target variable
X = df.drop('price', axis=1)
y = df['price']

# Step 4: Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Step 5: Preprocessing pipelines for numeric and categorical data
# Numeric features will be scaled
# Categorical features will be one-hot encoded
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ])

# Step 6: Split data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 7: Build a pipeline with preprocessing and a Linear Regression model as baseline
lr_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

# Train the Linear Regression model
lr_pipeline.fit(X_train, y_train)

# Predict on test data
y_pred_lr = lr_pipeline.predict(X_test)

# Evaluate the baseline model
print("Linear Regression Performance:")
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_lr)))
print("R2 Score:", r2_score(y_test, y_pred_lr))

# Step 8: Apply Ridge Regression with hyperparameter tuning
ridge_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', Ridge())
])

# Define hyperparameter grid for alpha (regularization strength)
param_grid_ridge = {'regressor__alpha': [0.01, 0.1, 1, 10, 100]}

# Use GridSearchCV to find the best alpha
grid_ridge = GridSearchCV(ridge_pipeline, param_grid_ridge, cv=5, scoring='neg_root_mean_squared_error')
grid_ridge.fit(X_train, y_train)
print("\nBest Ridge alpha:", grid_ridge.best_params_)

# Predict and evaluate Ridge model
y_pred_ridge = grid_ridge.predict(X_test)
print("Ridge Regression Performance:")
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_ridge)))  # <-- FIXED
print("R2 Score:", r2_score(y_test, y_pred_ridge))

# Step 9: Apply Lasso Regression with hyperparameter tuning
lasso_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', Lasso(max_iter=10000))
])

# Define hyperparameter grid for alpha
param_grid_lasso = {'regressor__alpha': [0.001, 0.01, 0.1, 1, 10]}

# Use GridSearchCV to find the best alpha
grid_lasso = GridSearchCV(lasso_pipeline, param_grid_lasso, cv=5, scoring='neg_root_mean_squared_error')
grid_lasso.fit(X_train, y_train)

print("\nBest Lasso alpha:", grid_lasso.best_params_)

# Predict and evaluate Lasso model
y_pred_lasso = grid_lasso.predict(X_test)
print("Lasso Regression Performance:")
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_lasso)))
print("R2 Score:", r2_score(y_test, y_pred_lasso))